This project is developed in fulfilment of the batch data processing module of the postgraduate diploma course in software development, specializing in big data.

The project aims to test the knowledge of various tools related to the batch procession. The batch Extraction, Transformation and Loading (ETL) pipeline is built using Apache Sqoop, Apache PySpark, Amazon Simple Storage Service (S3) and Amazon Redshift.

The task essentially is to build a batch ETL pipeline to read transactional data from Amazon Relational Database Service (RDS), transform it and then load it into Redshift tables to perform analytical queries on the loaded data.
